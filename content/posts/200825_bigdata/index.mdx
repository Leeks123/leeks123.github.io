---
title: <파이썬을 이용한 빅데이터 분석> ch10 PCA, LDA, NMF
date: 2020-08-25
tags:
  - Python
  - BigData
  - MachineLearning
---

### 차원 축소

- **차원** : 독립변수의 개수
- **차원의 저주**
    - 데이터의 차원이 커질수록 해당 공간을 설명하기 위한 데이터의 양이 증가.
    - 처음 가지고 있던 적은 데이터로 설명하기 때문에 과최적화 ⇒ 성능저하
- 차원 축소 방법
    - PCA (Pricipal Component Analysis), 주성분 분석 : 데이터 분포의 분산이 큰 축을 찾는 방법
    - LDA (Linear Discriminant Analysis), 선형 판별 분석 : 데이터의 클래스 정보를 유지하면서 분리하는 축을 찾는 방법

### PCA

데이터의 분산이 가장 큰 축들을 주성분으로 삼아서 데이터를 분석

- **주성분** : 고유벡터와 고유값을 사용해서 구함
    - **고유벡터** : 데이터의 분포를 나타내는 선
    - **고유값** : 해당 고유벡터에 데이터를 사상시켰을 때 데이터가 분포하는 분산

![pca](https://t1.daumcdn.net/cfile/tistory/996F65335B8A493207?download)

- 과정
    1. 데이터 셋에서 분산이 가장 큰 축을 찾음
    2. 첫번째 축과 직교하면서 분산이 가장 큰 두번째 축을 찾음
    3. 위와 같은 방법을 차원(특성의 수)만큼 찾음

### LDA

다음 조건을 만족하는 벡터를 찾는 작업

- 각 클래스의 중심간 거리가 최대인 벡터, 클래스간 분리를 잘 이뤄내는 벡터
- 각 클래스 내 데이터의 분산이 최소인 벡터

<img src="https://i.imgur.com/6ggd2F0.png" style="width : 900px">

---

### NMF

- Non-negative Matrix Factorization (비음수 행렬 인수분해)
- 행렬 V가 있을 때 W*H=V 두 행렬로 분해
    - W(가중치 행렬) : W의 각 열이 특성. 행은 V의 같은 행이 얼마나 적합한지, 가중치를 나타냄
    - H(특성 행렬) : H의 각 행이 특성. 열은 V의 같은 열이 특성에 얼마나 중요한지
- PCA는 최대 분산의 방향을 주성분이라 하여 성분 간의 우열이 있는 반면 
NMF는 특성이 양수이기만 하면 성분의 우열이 없이 특징을 나눌 수 있는 것이다